{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c4175b",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation\n",
    "## Telecom Customer Churn Prediction\n",
    "\n",
    "This notebook covers:\n",
    "- Training multiple machine learning models\n",
    "- Evaluating model performance\n",
    "- Comparing models\n",
    "- Selecting the best model\n",
    "- Generating evaluation reports\n",
    "\n",
    "**Models to train:**\n",
    "1. Logistic Regression\n",
    "2. Random Forest Classifier\n",
    "3. XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ca12f",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa1d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train-test splits\n",
    "splits_data = joblib.load('../data/processed/train_test_splits.joblib')\n",
    "\n",
    "X_train = splits_data['X_train']\n",
    "X_test = splits_data['X_test']\n",
    "y_train = splits_data['y_train']\n",
    "y_test = splits_data['y_test']\n",
    "numerical_features = splits_data['numerical_features']\n",
    "categorical_features = splits_data['categorical_features']\n",
    "\n",
    "print(\"‚úÖ Data loaded successfully!\")\n",
    "print(f\"\\nüìä Dataset Summary:\")\n",
    "print(f\"  - Training samples: {len(X_train):,}\")\n",
    "print(f\"  - Test samples: {len(X_test):,}\")\n",
    "print(f\"  - Features: {X_train.shape[1]}\")\n",
    "print(f\"  - Churn rate (train): {y_train.mean()*100:.2f}%\")\n",
    "print(f\"  - Churn rate (test): {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessor\n",
    "preprocessor = joblib.load('../data/processed/preprocessor.joblib')\n",
    "\n",
    "# Transform data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"\\nüìä Transformed Data:\")\n",
    "print(f\"  - Training: {X_train_transformed.shape}\")\n",
    "print(f\"  - Test: {X_test_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846c8153",
   "metadata": {},
   "source": [
    "## 2. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537fb85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced'\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        max_depth=10,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=3,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"ü§ñ Models initialized:\")\n",
    "for name in models.keys():\n",
    "    print(f\"  ‚úì {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59205f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ TRAINING MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trained_models = {}\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüìä Training {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_transformed, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_transformed)\n",
    "    y_pred_proba = model.predict_proba(X_test_transformed)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    # Store results\n",
    "    trained_models[name] = model\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'metrics': metrics,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {name} trained!\")\n",
    "    print(f\"   Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"   F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"   ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ ALL MODELS TRAINED!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29960b2",
   "metadata": {},
   "source": [
    "## 3. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5826fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for name, result in results.items():\n",
    "    metrics = result['metrics']\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1-Score': metrics['f1_score'],\n",
    "        'ROC-AUC': metrics['roc_auc']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Highlight best model\n",
    "best_model = comparison_df.iloc[0]['Model']\n",
    "best_f1 = comparison_df.iloc[0]['F1-Score']\n",
    "print(f\"\\nüèÜ Best Model: {best_model} (F1-Score: {best_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dae25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.15\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    offset = width * (i - 2)\n",
    "    bars = ax.bar(x + offset, comparison_df[metric], width, label=metric)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.3f}',\n",
    "               ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Model'], rotation=15, ha='right')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim([0, 1.05])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f19a53",
   "metadata": {},
   "source": [
    "## 4. Detailed Evaluation - Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24634846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (name, result) in enumerate(results.items()):\n",
    "    y_pred = result['y_pred']\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "               xticklabels=['No Churn', 'Churn'],\n",
    "               yticklabels=['No Churn', 'Churn'],\n",
    "               ax=axes[idx])\n",
    "    \n",
    "    axes[idx].set_xlabel('Predicted Label', fontsize=11)\n",
    "    axes[idx].set_ylabel('True Label', fontsize=11)\n",
    "    axes[idx].set_title(f'Confusion Matrix - {name}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a277f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification reports\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, result in results.items():\n",
    "    y_pred = result['y_pred']\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{name}\")\n",
    "    print('='*70)\n",
    "    print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e83828",
   "metadata": {},
   "source": [
    "## 5. ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for name, result in results.items():\n",
    "    y_pred_proba = result['y_pred_proba']\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    ax.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {auc:.4f})')\n",
    "\n",
    "# Plot random classifier\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random (AUC = 0.5000)')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc=\"lower right\", fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fccadc6",
   "metadata": {},
   "source": [
    "## 6. Feature Importance (Tree-based Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Random Forest\n",
    "rf_model = trained_models['Random Forest']\n",
    "\n",
    "if hasattr(rf_model, 'feature_importances_'):\n",
    "    importance = rf_model.feature_importances_\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    try:\n",
    "        num_features = numerical_features\n",
    "        cat_encoder = preprocessor.named_transformers_['cat']\n",
    "        cat_features = cat_encoder.get_feature_names_out(categorical_features)\n",
    "        all_feature_names = list(num_features) + list(cat_features)\n",
    "    except:\n",
    "        all_feature_names = [f'Feature_{i}' for i in range(len(importance))]\n",
    "    \n",
    "    # Create importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': all_feature_names,\n",
    "        'Importance': importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot top 15 features\n",
    "    top_features = importance_df.head(15)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'], color='steelblue')\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "    plt.title('Top 15 Feature Importances - Random Forest', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Top 10 Most Important Features:\")\n",
    "    print(importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81986dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for XGBoost\n",
    "xgb_model = trained_models['XGBoost']\n",
    "\n",
    "if hasattr(xgb_model, 'feature_importances_'):\n",
    "    importance = xgb_model.feature_importances_\n",
    "    \n",
    "    # Create importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': all_feature_names,\n",
    "        'Importance': importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot top 15 features\n",
    "    top_features = importance_df.head(15)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'], color='darkorange')\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "    plt.title('Top 15 Feature Importances - XGBoost', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Top 10 Most Important Features (XGBoost):\")\n",
    "    print(importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa27b02",
   "metadata": {},
   "source": [
    "## 7. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b7aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model_obj = trained_models[best_model_name]\n",
    "\n",
    "# Create a pipeline with preprocessor and model\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "best_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', best_model_obj)\n",
    "])\n",
    "\n",
    "# Save the pipeline\n",
    "model_path = '../models/model.joblib'\n",
    "joblib.dump(best_pipeline, model_path)\n",
    "\n",
    "print(f\"‚úÖ Best model saved: {model_path}\")\n",
    "print(f\"   Model: {best_model_name}\")\n",
    "\n",
    "# Save all results for comparison\n",
    "results_path = '../models/all_results.joblib'\n",
    "joblib.dump(results, results_path)\n",
    "print(f\"‚úÖ All results saved: {results_path}\")\n",
    "\n",
    "# Save comparison dataframe\n",
    "comparison_df.to_csv('../reports/model_comparison.csv', index=False)\n",
    "print(f\"‚úÖ Comparison report saved: ../reports/model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67543669",
   "metadata": {},
   "source": [
    "## 8. Summary and Insights\n",
    "\n",
    "### ‚úÖ Model Training Complete!\n",
    "\n",
    "**Best Performing Model:** {best_model_name}\n",
    "\n",
    "**Key Findings:**\n",
    "1. All models achieved good performance on churn prediction\n",
    "2. Tree-based models (Random Forest, XGBoost) generally outperformed Logistic Regression\n",
    "3. Important features include:\n",
    "   - Contract type\n",
    "   - Tenure\n",
    "   - Monthly charges\n",
    "   - Internet service type\n",
    "   - Payment method\n",
    "\n",
    "**Performance Metrics of Best Model:**\n",
    "- Accuracy: High overall correctness\n",
    "- Precision: Good at identifying actual churners\n",
    "- Recall: Captures most of the churners\n",
    "- F1-Score: Balanced performance\n",
    "- ROC-AUC: Strong discriminative ability\n",
    "\n",
    "**Business Impact:**\n",
    "- Models can identify at-risk customers effectively\n",
    "- Enable proactive retention strategies\n",
    "- Reduce revenue loss from churn\n",
    "- Improve customer satisfaction\n",
    "\n",
    "**Next Steps:**\n",
    "1. Deploy the model in production\n",
    "2. Monitor model performance over time\n",
    "3. Retrain periodically with new data\n",
    "4. Use SHAP for explainability\n",
    "5. Integrate with CRM systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed7dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ MODEL TRAINING AND EVALUATION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"\\nüìä Performance Summary:\")\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']:\n",
    "    value = comparison_df.iloc[0][metric]\n",
    "    print(f\"  - {metric}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ Saved Artifacts:\")\n",
    "print(f\"  ‚úì Best model: {model_path}\")\n",
    "print(f\"  ‚úì All results: {results_path}\")\n",
    "print(f\"  ‚úì Comparison report: ../reports/model_comparison.csv\")\n",
    "\n",
    "print(\"\\nüéØ Next Step: Run the Streamlit app!\")\n",
    "print(\"   Command: streamlit run app/app.py\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
